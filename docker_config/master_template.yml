readme_am: |
  *********************************
  Admin Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fl_admin.sh
  docker.sh

  Use the docker.sh script to start the admin console in the intended docker container.

  The rootCA.pem file is pointed by "ca_cert" in fl_admin.sh.  If you plan to move/copy it to a different place,
  you will need to modify fl_admin.sh.  The same applies to the other two files, client.crt and client.key.

  The email in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fc: |
  *********************************
  Federated Learning Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fed_client.json
  start.sh
  sub_start.sh
  stop_fl.sh
  docker.sh

  Use the docker.sh script with appropriate options to run preflight checks or to start the client in the intended docker container.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_client.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_client.json.  The same applies to the other two files, client.crt and client.key.

  The client name in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fs: |
  *********************************
  Federated Learning Server package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  server.crt
  server.key
  authorization.json
  fed_server.json
  start.sh
  sub_start.sh
  stop_fl.sh
  signature.json
  docker.sh

  Use the docker.sh script to start the server in the intended docker container.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_server.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_server.json.  The same applies to the other two files, server.crt and server.key.

  Please always safeguard the server.key.

gunicorn_conf_py: |
  bind="0.0.0.0:{~~port~~}"
  cert_reqs=2
  do_handshake_on_connect=True
  timeout=30
  worker_class="nvflare.ha.overseer.worker.ClientAuthWorker"
  workers=1
  wsgi_app="nvflare.ha.overseer.overseer:app"

local_client_resources: |
  {
    "format_version": 2,
    "client": {
      "retry_timeout": 30,
      "compression": "Gzip"
    },
    "components": [
      {
        "id": "resource_manager",
        "path": "nvflare.app_common.resource_managers.gpu_resource_manager.GPUResourceManager",
        "args": {
          "num_of_gpus": 0,
          "mem_per_gpu_in_GiB": 0
        }
      },
      {
        "id": "resource_consumer",
        "path": "nvflare.app_common.resource_consumers.gpu_resource_consumer.GPUResourceConsumer",
        "args": {}
      }
    ]
  }

fed_client: |
  {
    "format_version": 2,
    "servers": [
      {
        "name": "spleen_segmentation",
        "service": {
        }
      }
    ],
    "client": {
      "ssl_private_key": "client.key",
      "ssl_cert": "client.crt",
      "ssl_root_cert": "rootCA.pem"
    }
  }

sample_privacy: |
  {
    "scopes": [
      {
        "name": "public",
        "properties": {
          "train_dataset": "/data/public/train",
          "val_dataset": "/data/public/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.2,
              "max_noise_level": 0.2
            }
          },
          {
            "name": "PercentilePrivacy",
            "args": {
              "percentile": 10,
              "gamma": 0.02
            }
          }
        ],
        "task_data_filters": [
          {
            "name": "BadModelDetector"
          }
        ]
      },
      {
        "name": "private",
        "properties": {
          "train_dataset": "/data/private/train",
          "val_dataset": "/data/private/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.1,
              "max_noise_level": 0.1
            }
          },
          {
            "name": "SVTPrivacy",
            "args": {
              "fraction": 0.1,
              "epsilon": 0.2
            }
          }
        ]
      }
    ],
    "default_scope": "public"
  }

local_server_resources: |
  {
      "format_version": 2,
      "servers": [
          {
              "admin_storage": "transfer",
              "max_num_clients": 100,
              "heart_beat_timeout": 600,
              "num_server_workers": 4,
              "download_job_url": "http://download.server.com/",
              "compression": "Gzip"
          }
      ],
      "snapshot_persistor": {
          "path": "nvflare.app_common.state_persistors.storage_state_persistor.StorageStatePersistor",
          "args": {
              "uri_root": "/",
              "storage": {
                  "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage",
                  "args": {
                      "root_dir": "/tmp/nvflare/snapshot-storage",
                      "uri_root": "/"
                  }
              }
          }
      },
      "components": [
          {
              "id": "job_scheduler",
              "path": "nvflare.app_common.job_schedulers.job_scheduler.DefaultJobScheduler",
              "args": {
                  "max_jobs": 4
              }
          },
          {
              "id": "job_manager",
              "path": "nvflare.apis.impl.job_def_manager.SimpleJobDefManager",
              "args": {
                  "uri_root": "/tmp/nvflare/jobs-storage",
                  "job_store_id": "job_store"
              }
          },
          {
              "id": "job_store",
              "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage"
          }
      ]
  }

fed_server: |
  {
    "format_version": 2,
    "servers": [
        {
            "name": "spleen_segmentation",
            "service": {
                "target": "localhost:8002"
            },
            "admin_host": "localhost",
            "admin_port": 5005,
            "ssl_private_key": "server.key",
            "ssl_cert": "server.crt",
            "ssl_root_cert": "rootCA.pem"
        }
    ]
  }

fed_admin: |
  {
    "format_version": 1,
    "admin": {
      "with_file_transfer": true,
      "upload_dir": "transfer",
      "download_dir": "transfer",
      "with_login": true,
      "with_ssl": true,
      "cred_type": "cert",
      "client_key": "client.key",
      "client_cert": "client.crt",
      "ca_cert": "rootCA.pem",
      "prompt": "> "
    }
  }

default_authz: |
  {
    "format_version": "1.0",
    "permissions": {
      "project_admin": "any",
      "org_admin": {
        "submit_job": "none",
        "clone_job": "none",
        "manage_job": "o:submitter",
        "download_job": "o:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "none"
      },
      "lead": {
        "submit_job": "any",
        "clone_job": "n:submitter",
        "manage_job": "n:submitter",
        "download_job": "n:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "any"
      },
      "member": {
        "view": "any"
      }
    }
  }

authz_def: |
  {
    "rules": {
      "allow_byoc": {
        "desc": "allow BYOC in APP",
        "type": "bool",
        "default": false
      },
      "allow_custom_datalist": {
        "desc": "allow custom datalist",
        "default": true
      }
    },
    "rights": {
      "train_self": {
        "desc": "do training operations on own site",
        "default": false,
        "precond": "selfOrg"
      },
      "train_all": {
        "desc": "do training ops on all sites",
        "default": false
      },
      "view_self": {
        "desc": "view log files of own org",
        "default": true,
        "precond": "selfOrg"
      },
      "view_all": {
        "desc": "view log files of all sites",
        "default": false
      },
      "operate_all": {
        "desc": "start/stop all sites",
        "default": false
      },
      "operate_self": {
        "desc": "start/stop own site",
        "default": false,
        "precond": "selfOrg"
      }
    }
  }

fl_admin_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  python3 -m nvflare.fuel.hci.tools.admin -m $DIR/.. -s fed_admin.json

log_config: |
  [loggers]
  keys=root

  [handlers]
  keys=consoleHandler

  [formatters]
  keys=fullFormatter

  [logger_root]
  level=INFO
  handlers=consoleHandler

  [handler_consoleHandler]
  class=StreamHandler
  level=DEBUG
  formatter=fullFormatter
  args=(sys.stdout,)

  [formatter_fullFormatter]
  format=%(asctime)s - %(name)s - %(levelname)s - %(message)s

start_ovsr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  NVFL_OVERSEER_HEARTBEAT_TIMEOUT=10 AUTHZ_FILE=$DIR/privilege.yml gunicorn -c $DIR/gunicorn.conf.py --keyfile $DIR/overseer.key --certfile $DIR/overseer.crt --ca-certs $DIR/rootCA.pem

start_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  all_arguments="${@}"
  doCloud=false
  # parse arguments
  while [[ $# -gt 0 ]]
  do
      key="$1"
      case $key in
        --cloud)
          doCloud=true
          shift
        ;;
      esac
      shift
  done

  if [ $doCloud = true ]
  then
    echo "Only on-prem is currently supported."
  else
    $DIR/sub_start.sh &
  fi

start_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  all_arguments="${@}"
  doCloud=false
  ha_mode={~~ha_mode~~}
  # parse arguments
  while [[ $# -gt 0 ]]
  do
    key="$1"
    case $key in
      --cloud)
        if [ $ha_mode = false ]
        then
          doCloud=true
          shift
        else
          echo "Cloud launch does not support NVFlare HA mode."
          exit 1
        fi
      ;;
    esac
    shift
  done

  if [ $doCloud = true ]
  then
    echo "Only on-prem is currently supported."
  else
    $DIR/sub_start.sh &
  fi

stop_fl_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "Please use FL admin console to issue shutdown client command to properly stop this client."
  echo "This stop_fl.sh script can only be used as the last resort to stop this client."
  echo "It will not properly deregister the client to the server."
  echo "The client status on the server after this shell script will be incorrect."
  read -n1 -p "Would you like to continue (y/N)? " answer
  case $answer in
    y|Y)
      echo
      echo "Shutdown request created.  Wait for local FL process to shutdown."
      touch $DIR/../shutdown.fl
      ;;
    n|N|*)
      echo
      echo "Not continue"
      ;;
  esac

sub_start_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "WORKSPACE set to $DIR/.."
  mkdir -p $DIR/../transfer
  export PYTHONPATH=/local/custom:$PYTHONPATH
  echo "PYTHONPATH is $PYTHONPATH"

  SECONDS=0
  lst=-400
  restart_count=0
  start_fl() {
    if [[ $(( $SECONDS - $lst )) -lt 300 ]]; then
      ((restart_count++))
    else
      restart_count=0
    fi
    if [[ $(($SECONDS - $lst )) -lt 300 && $restart_count -ge 5 ]]; then
      echo "System is in trouble and unable to start the task!!!!!"
      rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl
      exit
    fi
    lst=$SECONDS
  ((python3 -u -m nvflare.private.fed.app.client.client_train -m $DIR/.. -s fed_client.json --set secure_train=true uid={~~client_name~~} org={~~org_name~~} config_folder={~~config_folder~~} 2>&1 & echo $! >&3 ) 3>$DIR/../pid.fl )
    pid=`cat $DIR/../pid.fl`
    echo "new pid ${pid}"
  }

  stop_fl() {
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "No pid.fl.  No need to kill process."
      return
    fi
    pid=`cat $DIR/../pid.fl`
    sleep 5
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      echo "Process already terminated"
      return
    fi
    kill -9 $pid
    rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl 2> /dev/null 1>&2
  }

  if [[ -f "$DIR/../daemon_pid.fl" ]]; then
    dpid=`cat $DIR/../daemon_pid.fl`
    kill -0 ${dpid} 2> /dev/null 1>&2
    if [[ $? -eq 0 ]]; then
      echo "There seems to be one instance, pid=$dpid, running."
      echo "If you are sure it's not the case, please kill process $dpid and then remove daemon_pid.fl in $DIR/.."
      exit
    fi
    rm -f $DIR/../daemon_pid.fl
  fi

  echo $BASHPID > $DIR/../daemon_pid.fl

  while true
  do
    sleep 5
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "start fl because of no pid.fl"
      start_fl
      continue
    fi
    pid=`cat $DIR/../pid.fl`
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      if [[ -f "$DIR/../shutdown.fl" ]]; then
        echo "Gracefully shutdown."
        break
      fi
      echo "start fl because process of ${pid} does not exist"
      start_fl
      continue
    fi
    if [[ -f "$DIR/../shutdown.fl" ]]; then
      echo "About to shutdown."
      stop_fl
      break
    fi
    if [[ -f "$DIR/../restart.fl" ]]; then
      echo "About to restart."
      stop_fl
    fi
  done

  rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl

sub_start_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "WORKSPACE set to $DIR/.."
  mkdir -p $DIR/../transfer

  SECONDS=0
  lst=-400
  restart_count=0
  start_fl() {
    if [[ $(( $SECONDS - $lst )) -lt 300 ]]; then
      ((restart_count++))
    else
      restart_count=0
    fi
    if [[ $(($SECONDS - $lst )) -lt 300 && $restart_count -ge 5 ]]; then
      echo "System is in trouble and unable to start the task!!!!!"
      rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl
      exit
    fi
    lst=$SECONDS
  ((python3 -u -m nvflare.private.fed.app.server.server_train -m $DIR/.. -s fed_server.json --set secure_train=true org={~~org_name~~} config_folder={~~config_folder~~} 2>&1 & echo $! >&3 ) 3>$DIR/../pid.fl )
    pid=`cat $DIR/../pid.fl`
    echo "new pid ${pid}"
  }

  stop_fl() {
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "No pid.fl.  No need to kill process."
      return
    fi
    pid=`cat $DIR/../pid.fl`
    sleep 5
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      echo "Process already terminated"
      return
    fi
    kill -9 $pid
    rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl
  }

  if [[ -f "$DIR/../daemon_pid.fl" ]]; then
    dpid=`cat $DIR/../daemon_pid.fl`
    kill -0 ${dpid} 2> /dev/null 1>&2
    if [[ $? -eq 0 ]]; then
      echo "There seems to be one instance, pid=$dpid, running."
      echo "If you are sure it's not the case, please kill process $dpid and then remove daemon_pid.fl in $DIR/.."
      exit
    fi
    rm -f $DIR/../daemon_pid.fl
  fi

  echo $BASHPID > $DIR/../daemon_pid.fl

  while true
  do
    sleep 5
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "start fl because of no pid.fl"
      start_fl
      continue
    fi
    pid=`cat $DIR/../pid.fl`
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      if [[ -f "$DIR/../shutdown.fl" ]]; then
        echo "Gracefully shutdown."
        break
      fi
      echo "start fl because process of ${pid} does not exist"
      start_fl
      continue
    fi
    if [[ -f "$DIR/../shutdown.fl" ]]; then
      echo "About to shutdown."
      stop_fl
      break
    fi
    if [[ -f "$DIR/../restart.fl" ]]; then
      echo "About to restart."
      stop_fl
    fi
  done

  rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl

docker_cln_sh: |
  #!/usr/bin/env bash
  # docker run script for FL client with proper env variable forwarding
  # Auto disable TTY in non-interactive CI environments
  if [ -t 1 ]; then
      TTY_OPT="-it"
  else
      echo "[INFO] No interactive terminal detected, disabling TTY."
      TTY_OPT=""
  fi

  # Parse command-line arguments
  while [[ "$#" -gt 0 ]]; do
      case $1 in
          --data_dir)        MY_DATA_DIR="$2"; shift ;;
          --scratch_dir)     MY_SCRATCH_DIR="$2"; shift ;;
          --GPU)             GPU2USE="$2"; shift ;;
          --no_pull)         NOPULL="1" ;;
          --dummy_training)  DUMMY_TRAINING="1" ;;
          --preflight_check) PREFLIGHT_CHECK="1" ;;
          --local_training)  LOCAL_TRAINING="1" ;;
          --start_client)    START_CLIENT="1" ;;
          --interactive)     INTERACTIVE="1" ;;
          *) echo "Unknown parameter passed: $1"; exit 1 ;;
      esac
      shift
  done

  # Prompt for parameters if missing
  if [[ -z "$DUMMY_TRAINING" && -z "$MY_DATA_DIR" ]]; then
      read -p "Enter the path to your data directory (default: /home/flclient/data): " user_data_dir
      : ${MY_DATA_DIR:="${user_data_dir:-/home/flclient/data}"}
  fi

  if [ -z "$MY_SCRATCH_DIR" ]; then
      read -p "Enter the path to your scratch directory (default: /mnt/scratch): " user_scratch_dir
      : ${MY_SCRATCH_DIR:="${user_scratch_dir:-/mnt/scratch}"}
  fi

  if [ -z "$GPU2USE" ]; then
      read -p "Enter the GPU index to use or 'all' (default: device=0): " user_gpu
      : ${GPU2USE:="${user_gpu:-device=0}"}
  fi

  # Resolve script directory
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

  if [ -t 1 ]; then
      # Local only
      sudo mkdir -p "$MY_SCRATCH_DIR"
      sudo chown -R $(id -u):$(id -g) "$MY_SCRATCH_DIR"
      sudo chmod -R 777 "$MY_SCRATCH_DIR"
  else
      mkdir -p "$MY_SCRATCH_DIR"
      chmod -R 777 "$MY_SCRATCH_DIR"
  fi


  # Networking & Cleanup
  NETARG="--net=host"
  rm -rf ../pid.fl ../daemon_pid.fl

  # Docker image and container name
  DOCKER_IMAGE={~~docker_image~~}
  if [ -z "$NOPULL" ]; then
      echo "Updating docker image"
      docker pull "$DOCKER_IMAGE"
  fi

  CONTAINER_NAME=odelia_swarm_client_{~~client_name~~}
  DOCKER_OPTIONS_A="--name=$CONTAINER_NAME --gpus=$GPU2USE -u $(id -u):$(id -g)"
  DOCKER_MOUNTS="-v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/startupkit/ -v $MY_SCRATCH_DIR:/scratch/"
  if [[ ! -z "$MY_DATA_DIR" ]]; then
      DOCKER_MOUNTS+=" -v $MY_DATA_DIR:/data/:ro"
  fi
  DOCKER_OPTIONS_B="-w /startupkit/startup/ --ipc=host $NETARG"
  DOCKER_OPTIONS="${DOCKER_OPTIONS_A} ${DOCKER_MOUNTS} ${DOCKER_OPTIONS_B}"

  # Common ENV vars
  ENV_VARS="--env SITE_NAME={~~client_name~~} \
            --env DATA_DIR=/data \
            --env SCRATCH_DIR=/scratch \
            --env TORCH_HOME=/scratch \
            --env GPU_DEVICE=$GPU2USE \
            --env INSTITUTION={~~client_name~~} \
            --env MODEL_NAME=ResNet10 \
            --env CONFIG=unilateral"

  # Execution modes
  if [[ ! -z "$DUMMY_TRAINING" ]]; then
      docker run --rm $TTY_OPT $DOCKER_OPTIONS $ENV_VARS --env TRAINING_MODE=local_training $DOCKER_IMAGE \
      /bin/bash -c "/MediSwarm/application/jobs/minimal_training_pytorch_cnn/app/custom/main.py"

  elif [[ ! -z "$PREFLIGHT_CHECK" ]]; then
      docker run --rm $TTY_OPT $DOCKER_OPTIONS $ENV_VARS --env TRAINING_MODE=preflight_check --env NUM_EPOCHS=1 $DOCKER_IMAGE \
      /bin/bash -c "/MediSwarm/application/jobs/3dcnn_ptl/app/custom/main.py"

  elif [[ ! -z "$LOCAL_TRAINING" ]]; then
      docker run --rm $TTY_OPT $DOCKER_OPTIONS $ENV_VARS --env TRAINING_MODE=local_training --env NUM_EPOCHS=100 $DOCKER_IMAGE \
      /bin/bash -c "/MediSwarm/application/jobs/3dcnn_ptl/app/custom/main.py"

  elif [[ ! -z "$START_CLIENT" ]]; then
      docker run -d -t --rm $DOCKER_OPTIONS $ENV_VARS --env TRAINING_MODE=swarm $DOCKER_IMAGE \
      /bin/bash -c "nohup ./start.sh >> nohup.out 2>&1 && /bin/bash"

  elif [[ ! -z "$INTERACTIVE" ]]; then
      docker run --rm $TTY_OPT --detach-keys="ctrl-x" $DOCKER_OPTIONS $DOCKER_IMAGE /bin/bash

  else
      echo "❗ One of the following options must be passed:"
      echo "--dummy_training   minimal sanity check for Docker/GPU"
      echo "--preflight_check  verify data access & local training"
      echo "--local_training   train a local model"
      echo "--start_client     launch FL client in swarm mode"
      echo "--interactive      drop into interactive container"
      exit 1
  fi



docker_svr_sh: |
  #!/usr/bin/env bash
  # docker run script for FL server

  while [[ "$#" -gt 0 ]]; do
      case $1 in
          --no_pull)        NOPULL="1";;
          --start_server)   START_SERVER="1";;
          --list_licenses)  LIST_LICENSES="1";;
          --interactive)    INTERACTIVE="1";;
          *) echo "Unknown parameter passed: $1"; exit 1 ;;
      esac
      shift
  done

  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

  # to use host network, use line below
  NETARG="--net=host"
  # or to expose specific ports, use line below
  #NETARG="-p {~~admin_port~~}:{~~admin_port~~} -p {~~fed_learn_port~~}:{~~fed_learn_port~~}"

  DOCKER_IMAGE={~~docker_image~~}
  if [ -z "$NOPULL" ]; then
      echo "Updating docker image"
      docker pull $DOCKER_IMAGE
  fi
  svr_name="${SVR_NAME:-flserver}"
  CONTAINER_NAME=odelia_swarm_server_$svr_name

  rm -rf ../pid.fl ../daemon_pid.fl  # clean up potential leftovers from previous run

  echo "Starting docker with $DOCKER_IMAGE as $CONTAINER_NAME"
  # Run docker with appropriate parameters
  if [ ! -z "$START_SERVER" ]; then
      docker run -d -t --rm --name=$CONTAINER_NAME \
      -v $DIR/..:/startupkit/ -w /startupkit/startup/ \
      --ipc=host $NETARG $DOCKER_IMAGE \
      /bin/bash -c "nohup ./start.sh >> nohup.out 2>&1 && chmod a+r nohup.out && /bin/bash"
  elif [ ! -z "$LIST_LICENSES" ]; then
      docker run -it --rm --name=$CONTAINER_NAME \
      $DOCKER_IMAGE \
      /bin/bash -c "pip-licenses -s -u --order=license"
  elif [ ! -z "$INTERACTIVE" ]; then
      docker run --rm -it --detach-keys="ctrl-x" --name=$CONTAINER_NAME \
      -v $DIR/..:/startupkit/ -w /startupkit/startup/ \
      --ipc=host $NETARG $DOCKER_IMAGE \
      /bin/bash -c "/bin/bash"
  else
      echo "One of the following options must be passed:"
      echo "--start_server     start the swarm learning server"
      echo "--list_licenses    list licenses of installed python packages"
      echo "--interactive      start the container with an interactive shell (for debugging purposes)"
  fi

docker_adm_sh: |
  #!/usr/bin/env bash

  while [[ "$#" -gt 0 ]]; do
      case $1 in
         --no_pull)         NOPULL="1";;
          *) echo "Unknown parameter passed: $1"; exit 1 ;;
      esac
      shift
  done

  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # To use host network
  NETARG="--net=host"

  DOCKER_IMAGE={~~docker_image~~}
  if [ -z "$NOPULL" ]; then
      echo "Updating docker image"
      docker pull $DOCKER_IMAGE
  fi
  CONTAINER_NAME=odelia_swarm_admin

  echo "Starting docker with $DOCKER_IMAGE as $CONTAINER_NAME"
  docker run --rm -it --name=fladmin -v $DIR/../local/:/fl_admin/local/ -v $DIR/../startup/:/fl_admin/startup/ -w /fl_admin/startup/ $NETARG $DOCKER_IMAGE /bin/bash -c "./fl_admin.sh"

compose_yaml: |
  services:
    __overseer__:
      build: ./nvflare
      image: ${IMAGE_NAME}
      volumes:
        - .:/workspace
      command: ["${WORKSPACE}/startup/start.sh"]
      ports:
        - "8443:8443"

    __flserver__:
      image: ${IMAGE_NAME}
      ports:
        - "8002:8002"
        - "8003:8003"
      volumes:
        - .:/workspace
        - nvflare_svc_persist:/tmp/nvflare/
      command: ["${PYTHON_EXECUTABLE}",
            "-u",
            "-m",
            "nvflare.private.fed.app.server.server_train",
            "-m",
            "${WORKSPACE}",
            "-s",
            "fed_server.json",
            "--set",
            "secure_train=true",
            "config_folder=config",
            "org=__org_name__",
          ]

    __flclient__:
      image: ${IMAGE_NAME}
      volumes:
        - .:/workspace
      command: ["${PYTHON_EXECUTABLE}",
            "-u",
            "-m",
            "nvflare.private.fed.app.client.client_train",
            "-m",
            "${WORKSPACE}",
            "-s",
            "fed_client.json",
            "--set",
            "secure_train=true",
            "uid=__flclient__",
            "org=__org_name__",
            "config_folder=config",
          ]

  volumes:
    nvflare_svc_persist:

dockerfile: |
  RUN pip install -U pip
  RUN pip install nvflare
  COPY requirements.txt requirements.txt
  RUN pip install -r requirements.txt

helm_chart_chart: |
  apiVersion: v2
  name: nvflare
  description: A Helm chart for NVFlare overseer and servers
  type: application
  version: 0.1.0
  appVersion: "2.2.0"

helm_chart_service_overseer: |
  apiVersion: v1
  kind: Service
  metadata:
    name: overseer
  spec:
    selector:
      system: overseer
    ports:
      - protocol: TCP
        port: 8443
        targetPort: overseer-port

helm_chart_service_server: |
  apiVersion: v1
  kind: Service
  metadata:
    name: server
    labels:
      system: server
  spec:
    selector:
      system: server
    ports:
      - name: fl-port
        protocol: TCP
        port: 8002
        targetPort: fl-port
      - name: admin-port
        protocol: TCP
        port: 8003
        targetPort: admin-port

helm_chart_deployment_overseer: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: overseer
    labels:
      system: overseer
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: overseer
    template:
      metadata:
        labels:
          system: overseer
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
        containers:
          - name: overseer
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
            command: ["/workspace/overseer/startup/start.sh"]
            ports:
              - name: overseer-port
                containerPort: 8443
                protocol: TCP
helm_chart_deployment_server: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: server
    labels:
      system: server
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: server
    template:
      metadata:
        labels:
          system: server
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
          - name: persist
            hostPath:
              path: /tmp/nvflare
              type: Directory
        containers:
          - name: server1
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
              - name: persist
                mountPath: /tmp/nvflare
            command: ["/usr/local/bin/python3"]
            args:
              [
                "-u",
                "-m",
                "nvflare.private.fed.app.server.server_train",
                "-m",
                "/workspace/server",
                "-s",
                "fed_server.json",
                "--set",
                "secure_train=true",
                "config_folder=config",
                "org=__org_name__",

              ]
            ports:
              - containerPort: 8002
                protocol: TCP
              - containerPort: 8003
                protocol: TCP
helm_chart_values: |
  workspace: /home/nvflare
  persist: /home/nvflare


cloud_script_header: |
  #!/usr/bin/env bash

  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  function report_status() {
    status="$1"
    if [ "${status}" -ne 0 ]
    then
      echo "$2 failed"
      exit "${status}"
    fi
  }

  function check_binary() {
    echo -n "Checking if $1 exists. => "
    if ! command -v $1 &> /dev/null
    then
      echo "not found. $2"
      exit 1
    else
      echo "found"
    fi
  }

  function prompt() {
    local __default="$1"
    read -p "$2" ans
    if [[ ! -z "$ans" ]]
    then
      eval $__default="'$ans'"
    fi
  }

  # parse arguments
  while [[ $# -gt 0 ]]
  do
    key="$1"
    case $key in
      --config)
        config_file=$2
        shift
      ;;
      --image)
        image_name=$2
        shift
      ;;
      --vpc-id)
        vpc_id=$2
        shift
      ;;
      --subnet-id)
        subnet_id=$2
        shift
      ;;
    esac
    shift
  done

adm_notebook: |
  {
  "cells": [
    {
    "cell_type": "markdown",
    "id": "b758695b",
    "metadata": {},
    "source": [
      "# System Info"
    ]
    },
    {
    "cell_type": "markdown",
    "id": "9f7cd9e6",
    "metadata": {},
    "source": [
      "In this notebook, System Info is checked with the FLARE API."
    ]
    },
    {
    "cell_type": "markdown",
    "id": "ea50ba28",
    "metadata": {},
    "source": [
      "#### 1. Connect to the FL System with the FLARE API\n",
      "\n",
      "Use `new_secure_session()` to initiate a session connecting to the FL Server with the FLARE API. The necessary arguments are the username of the admin user you are using and the corresponding startup kit location.\n",
      "\n",
      "In the code example below, we get the `admin_user_dir` by concatenating the workspace root with the default directories that are created if you provision a project with a given project name. You can change the values to what applies to your system if needed.\n",
      "\n",
      "Note that if debug mode is not enabled, there is no output after initiating a session successfully, so instead we print the output of `get_system_info()`. If you are unable to connect and initiate a session, make sure that your FL Server is running and that the configurations are correct with the right path to the admin startup kit directory."
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "0166942d",
    "metadata": {
      "collapsed": true
    },
    "outputs": [],
    "source": [
      "# Run this pip install if NVFlare is not installed in your Jupyter Notebook\n",
      "\n",
      "# !python3 -m pip install -U nvflare"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "c3dbde69",
    "metadata": {},
    "outputs": [],
    "source": [
      "import os\n",
      "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
      "\n",
      "username = \"{~~admin_name~~}\"  # change this to your own username\n",
      "\n",
      "sess = new_secure_session(\n",
      "    username=username,\n",
      "    startup_kit_location=os.getcwd()\n",
      ")\n",
      "print(sess.get_system_info())"
    ]
    },
    {
    "cell_type": "markdown",
    "id": "31ccb6a6",
    "metadata": {},
    "source": [
      "### 2. Shutting Down the FL System\n",
      "\n",
      "As of now, there is no specific FLARE API command for shutting down the FL system, but the FLARE API can use the `do_command()` function of the underlying AdminAPI to submit any commands that the FLARE Console supports including shutdown commands to the clients and server:"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "b0d8aa9c",
    "metadata": {},
    "outputs": [],
    "source": [
      "print(sess.api.do_command(\"shutdown client\"))\n",
      "print(sess.api.do_command(\"shutdown server\"))\n",
      "\n",
      "sess.close()"
    ]
    }
  ],
  "metadata": {
    "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
    },
    "language_info": {
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.13"
    },
    "vscode": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
  }
